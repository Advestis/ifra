from copy import deepcopy
from datetime import datetime
from pathlib import Path
from time import time, sleep
from typing import List, Union

from ruleskit import RuleSet
from transparentpath import TransparentPath
import logging

from ifra.configs import NodeLearningConfig, CentralLearningConfig

logger = logging.getLogger(__name__)


class NodeGate:
    node_config_paths = "configs.json"
    instances = 0

    def __init__(self, path):
        self.id = NodeGate.instances
        self.path = path
        self.learning_configs = None
        self.ruleset = None
        self.last_fetch = None
        self.new_data = False
        NodeGate.instances += 1

    def interact(self):
        def get_ruleset():
            self.ruleset = RuleSet()
            self.ruleset.load(self.learning_configs.local_model_path)
            self.last_fetch = datetime.now()
            logger.info(f"Fetched new ruleset from node {self.id} at {self.last_fetch}")
            self.new_data = True

        if self.learning_configs is None:
            config_path = self.path / NodeGate.node_config_paths
            if not config_path.isfile():
                return
            self.learning_configs = NodeLearningConfig(config_path)
            if self.learning_configs.id is None:
                self.learning_configs.id = self.id
                self.learning_configs.save()

        if self.ruleset is None:
            if self.learning_configs.local_model_path.isfile():
                get_ruleset()
        else:
            if (
                self.learning_configs.local_model_path.isfile()
                and self.learning_configs.local_model_path.info()["mtime"] > self.last_fetch.timestamp()
            ):
                get_ruleset()

    def push_central_model(self, ruleset):
        logger.info(f"Pushing central model to node {self.id} at {self.learning_configs.central_model_path}")
        ruleset.save(self.learning_configs.central_model_path)
        self.new_data = False


class CentralServer:
    """Implementation of the notion of central server in federated learning.

    It monitors changes in a given list of remote GCP directories, were nodes are expected to write their models.
    Upon changes of the model files, the central server download them. Each time a new model file is downloaded, the
    central model is updated and sent in another GCP directory. This directory should be monitored by each node.
    """

    def __init__(
        self,
        nodes_paths: List[TransparentPath],
        central_configs_path: Union[str, Path, TransparentPath],
    ):
        self.reference_node_config = None
        if type(central_configs_path) == str:
            central_configs_path = Path(central_configs_path)
        self.central_configs = CentralLearningConfig(central_configs_path)
        self.nodes_configs = {}
        self.nodes = [NodeGate(path) for path in nodes_paths]
        self.rulesets = []
        self.ruleset = None

    def aggregate(self):
        """Among all rules generated by the nodes in the current iteration, will keep only the most recurent rule(s).

        Those rules will be passed to the node later, and the points associated to them will be ignored in order to find
        other relevant rules. In order not to remove too many points, a maximum of coverage is imposed.

        Returns False if no rules were found by nodes, which will stop the learning iterations.
        """
        logger.info("Aggregating fit results...")
        all_rules = []
        for ruleset in self.rulesets:
            all_rules += ruleset.rules
        if len(all_rules) == 0:
            logger.info("... no rules found")
            return
        occurences = {r: all_rules.count(r) for r in set(all_rules) if r.coverage < self.central_configs.max_coverage}
        if len(occurences) == 0:
            logger.warning("No rules matched coverage criterion")
            return
        max_occurences = max(list(occurences.values()))
        if self.ruleset is None:
            self.ruleset = RuleSet(
                [r for r in occurences if occurences[r] == max_occurences],
                remember_activation=False,
                stack_activation=False,
            )
        else:
            new_rules = [r for r in occurences if occurences[r] == max_occurences and r not in self.ruleset]
            if len(new_rules) == 0:
                logger.warning("No new rules found")
                return
            self.ruleset += RuleSet(
                new_rules,
                remember_activation=False,
                stack_activation=False,
            )
        logger.info("... fit results aggregated")

    def watch(self, timeout: int = 60, sleeptime: int = 5):
        t = time()
        updated_nodes = []
        while time() - t < timeout:
            new_models = False

            for node in self.nodes:
                if node in updated_nodes:
                    continue
                node.interact()  # Node fetches its latest data from GCP

                if node not in self.nodes_configs:
                    if node.learning_configs is not None:
                        if self.reference_node_config is None:
                            self.reference_node_config = deepcopy(node.learning_configs)
                        # Nodes with configurations different from central's are ignored
                        elif node.learning_configs != self.reference_node_config:
                            logger.warning(
                                f"Node {node.id}'s configuration is not compatible with previous"
                                " configuration. Ignoring."
                            )
                            continue
                        self.nodes_configs[node] = node.learning_configs
                    # Nodes with no available configurations are ignored
                    else:
                        logger.warning(f"Node {node.id}'s has no configurations. Ignoring.")
                        continue
                else:
                    # Nodes with no available configurations are ignored
                    if node.learning_configs is None:
                        logger.warning(f"Node {node.id} lost its configuration file. Ignoring.")
                        continue
                    # Nodes with configurations that changed are ignored
                    if node.learning_configs != self.nodes_configs[node]:
                        logger.warning(f"Node {node.id} changed its configurations. Ignoring.")
                        continue
                    # Nodes with configurations different from central's are ignored
                    if node.learning_configs != self.reference_node_config:
                        logger.warning(
                            f"Node {node.id}'s configuration is not compatible with previous"
                            " configuration. Ignoring."
                        )
                        continue

                if node.new_data:
                    updated_nodes.append(node)
                    self.rulesets.append(node.ruleset)
                    if len(updated_nodes) >= self.central_configs.min_number_of_new_models:
                        new_models = True

            if new_models:
                logger.info("Found enough new nodes nodels.")
                self.aggregate()
                self.rulesets = []
                updated_nodes = []
                for node in self.nodes:
                    node.push_central_model(deepcopy(self.ruleset))
                if self.ruleset is None:
                    logger.warning("No rules were found, no output generated.")
                else:
                    iteration = 0
                    name = self.central_configs.output_path.stem
                    path = self.central_configs.output_path.parent / f"{name}_{iteration}.csv"
                    while path.isfile():
                        iteration += 1
                        path = path.parent / f"{name}_{iteration}.csv"
                    self.ruleset.save(self.central_configs.output_path)
                    self.ruleset.save(path)
            sleep(sleeptime)

        logger.info(f"Timeout of {timeout} seconds reached, stopping learning.")
        logger.info(f"Results saved in {self.central_configs.output_path}")
