from copy import deepcopy
from datetime import datetime
from pathlib import Path
from time import time, sleep
from typing import List, Union

from ruleskit import RuleSet
from transparentpath import TransparentPath
import logging

logger = logging.getLogger(__name__)


class Config:

    """Class to load a node configuration on its local computer. Basically just a wrapper around a json file."""

    EXPECTED_CONFIGS = []

    # noinspection PyUnresolvedReferences
    def __init__(self, path: Union[str, Path, TransparentPath]):

        if type(path) == str:
            path = TransparentPath(path)

        if not path.suffix == ".json":
            raise ValueError("Config path must be a json")

        if hasattr(path, "read"):
            self.configs = path.read()
        else:
            with open(path) as opath:
                self.configs = load(opath)

        for key in self.EXPECTED_CONFIGS:
            if key not in self.configs:
                raise IndexError(f"Missing required config {key}")
        for key in self.configs:
            if key not in self.EXPECTED_CONFIGS:
                raise IndexError(f"Unexpected config {key}")

        for key in self.configs:
            if self.configs[key] == "":
                self.configs[key] = None

    def __getattr__(self, item):
        if item not in self.configs:
            raise ValueError(f"No configuration named '{item}' was found")
        return self.configs[item]


class NodeLearningConfig(Config):
    EXPECTED_CONFIGS = [
        "features_names",
        "classes_names",
        "x_mins",
        "x_maxs",
        "max_depth",
        "remember_activation",
        "stack_activation",
        "plot_data",
        "get_leaf",
        "local_model_path",
        "central_model_path"
    ]

    def __eq__(self, other):
        if not isinstance(other, NodeLearningConfig):
            return False
        for key in NodeLearningConfig.EXPECTED_CONFIGS:
            if key == "local_model_path":  # This parameter can change without problem
                continue
            if self[key] != other[key]:
                return False
        return True


class CentralLearningConfig(Config):
    EXPECTED_CONFIGS = [
        "max_coverage",
        "output_path",
    ]


class Node:
    node_config_paths = "configs.json"
    instances = 0

    def __init__(self, path):
        self.id = Node.instances
        self.path = path
        self.config = None
        self.ruleset = None
        self.last_fetch = None
        self.new_data = False
        Node.instances += 1

    def interact(self):
        def get_ruleset():
            self.ruleset = RuleSet()
            self.ruleset.load(self.config.local_model_path)
            self.last_fetch = datetime.now()
            logger.info(f"Fetched new ruleset from node {self.id} at {self.last_fetch}")
            self.new_data = True

        if self.config is None:
            config_path = self.path / Node.node_config_paths
            if not config_path.isfile():
                return
            self.config = NodeLearningConfig(config_path)
        if self.ruleset is None:
            if self.config.output_path.isfile():
                get_ruleset()
        else:
            if (
                self.config.local_model_path.isfile()
                and self.config.local_model_path.info()["mtime"] > self.last_fetch.timestamp()
            ):
                get_ruleset()

    def push_central_model(self, ruleset):
        logger.info(f"Pushing central model to node {self.id} at {self.config.central_model_path}")
        ruleset.save(self.config.central_model_path)
        self.new_data = False


class CentralServer:
    """Implementation of the notion of central server in federated learning.

    It monitors changes in a given list of remote GCP directories, were nodes are expected to write their models.
    Upon changes of the model files, the central server download them. Each time a new model file is downloaded, the
    central model is updated and sent in another GCP directory. This directory should be monitored by each node.
    """

    def __init__(self, nodes_paths: List[TransparentPath], central_configs_path: Union[str, Path, TransparentPath]):
        self.reference_node_config = None
        if type(central_configs_path) == str:
            central_configs_path = Path(central_configs_path)
        self.central_configs = CentralLearningConfig(central_configs_path)
        self.nodes_configs = {}
        self.nodes = [Node(path) for path in nodes_paths]
        self.rulesets = []
        self.ruleset = None

    def aggregate(self):
        """Among all rules generated by the nodes in the current iteration, will keep only the most recurent rule(s).

        Those rules will be passed to the node later, and the points associated to them will be ignored in order to find
        other relevant rules. In order not to remove too many points, a maximum of coverage is imposed.

        Returns False if no rules were found by nodes, which will stop the learning iterations.
        """
        logger.info("Aggregating fit results...")
        all_rules = []
        for ruleset in self.rulesets:
            all_rules += ruleset.rules
        if len(all_rules) == 0:
            logger.info("... no rules found")
            return
        occurences = {r: all_rules.count(r) for r in set(all_rules) if r.coverage < self.central_configs.max_coverage}
        max_occurences = max(list(occurences.values()))
        if self.ruleset is None:
            self.ruleset = RuleSet(
                [r for r in occurences if occurences[r] == max_occurences],
                remember_activation=False,
                stack_activation=False,
            )
        else:
            self.ruleset += RuleSet(
                [r for r in occurences if occurences[r] == max_occurences],
                remember_activation=False,
                stack_activation=False,
            )
        logger.info("... fit results aggregated")
        return

    def watch(self, timeout: int = 60):
        t = time()
        while time() - t < timeout:
            updated_nodes = []

            for node in self.nodes:
                node.interact()  # Node fetches its latest data from GCP

                if node not in self.nodes_configs:
                    if node.config is not None:
                        if self.reference_node_config is None:
                            self.reference_node_config = deepcopy(node.config)
                        # Nodes with configurations different from central's are ignored
                        elif node.config != self.reference_node_config:
                            logger.warning(f"Node {node.id}'s configuration is not compatible with previous"
                                           " configuration. Ignoring.")
                            continue
                        self.nodes_configs[node] = node.config
                    # Nodes with no available configurations are ignored
                    else:
                        logger.warning(f"Node {node.id}'s has no configurations. Ignoring.")
                        continue
                else:
                    # Nodes with no available configurations are ignored
                    if node.config is None:
                        logger.warning(f"Node {node.id} lost its configuration file. Ignoring.")
                        continue
                    # Nodes with configurations that changed are ignored
                    if node.config != self.nodes_configs[node]:
                        logger.warning(f"Node {node.id} changed its configurations. Ignoring.")
                        continue
                    # Nodes with configurations different from central's are ignored
                    if node.config != self.reference_node_config:
                        logger.warning(f"Node {node.id}'s configuration is not compatible with previous"
                                       " configuration. Ignoring.")
                        continue

                if node.new_data:
                    updated_nodes.append(node)
                    self.rulesets.append(node.ruleset)

            self.aggregate()
            self.rulesets = []
            for node in self.nodes:
                node.push_central_model(deepcopy(self.ruleset))
            sleep(5)

        if self.ruleset is None:
            logger.warning("No rules were found, no output generated.")
        else:
            self.ruleset.save(self.central_configs.output_path)
        logger.info(f"...fit finished, results saved in {self.central_configs.output_path}")
