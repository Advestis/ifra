<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>ifra.node API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ifra.node</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import random
from datetime import datetime
from time import time, sleep

from typing import Union, Tuple

import numpy as np
import pandas as pd
from ruleskit import RuleSet, RegressionRule, ClassificationRule
from tablewriter import TableWriter
from transparentpath import TransparentPath
import logging

from .actor import Actor
from .configs import NodeLearningConfig, NodeDataConfig
from .diff_privacy import apply_diff_privacy
from .fitters import DecisionTreeClassificationFitter, Fitter, DecisionTreeRegressionFitter
from .loader import load_y
from .node_model_updaters import AdaBoostNodeModelUpdater, NodeModelUpdater
from .datapreps import BinFeaturesDataPrep, DataPrep
from .train_test_split import TrainTestSplit
from .plot import plot_histogram
from .decorator import emit

logger = logging.getLogger(__name__)


# noinspection PyAttributeOutsideInit
class Node(Actor):
    # noinspection PyUnresolvedReferences
    &#34;&#34;&#34;One node of federated learning.
    This class should be used by each machine that is supposed to produce a local
    model. It monitors changes in a directory where the central server is supposed to push its model, and triggers fit
    when a new central model is available. It will also trigger a first fit before any central model is available. It
    also monitors changes in the input data file, to automatically re-fit the model when new data are available.
    Any rules produced by the node and already present in the central model are ignored.

    Attributes
    ----------
    learning_configs: `ifra.configs.NodeLearningConfig`
        The learning configuration of the node. Will be accessible by the central server.
        see `ifra.configs.NodeLearningConfig`
    path_learning_configs: TransparentPath
        Path to the json file containing learning configuration of the node. Needs to be kept in memory to potentially
        update the node&#39;s id once set by the central server.
    emitter: Emitter
        `ifra.messenger.Emitter`
    dataprep: Union[None, Callable]
        The dataprep method to apply to the data before fitting. Optional, specified in the node&#39;s learning
        configuration.
    datapreped: bool
        False if the dataprep has not been done yet, True after
    splitted: bool
        This flag is True if the features and targets have been splitted into train/test yet, False otherwise.
    model: Union[None, RuleSet]
        Fitted model
    last_fetch: Union[None, datetime]
        Date and time when the central model was last fetched.
    last_x: Union[None, datetime]
        Date and time when the x data were last modified.
    last_y: Union[None, datetime]
        Date and time when the y data were last modified.
    data: `ifra.configs.NodeDataConfig`
        Configuration of the paths to the node&#39;s features and target data files
    fitter: `ifra.fitters.Fitter`
        Configuration of the paths to the node&#39;s features and target data files
    updater: `ifra.node_model_updaters.NodeModelUpdater`
        Configuration of the paths to the node&#39;s features and target data files
    &#34;&#34;&#34;

    possible_fitters = {
        &#34;decisiontreeclassification&#34;: DecisionTreeClassificationFitter,
        &#34;decisiontreeregression&#34;: DecisionTreeRegressionFitter,
    }
    &#34;&#34;&#34;Possible string values and corresponding fitter object for *fitter* attribute of
    `ifra.configs.NodeLearningConfig`&#34;&#34;&#34;

    possible_updaters = {&#34;adaboost&#34;: AdaBoostNodeModelUpdater}
    &#34;&#34;&#34;Possible string values and corresponding updaters object for *updater* attribute of
    `ifra.configs.NodeLearningConfig`&#34;&#34;&#34;

    possible_datapreps = {&#34;binfeatures&#34;: BinFeaturesDataPrep}
    &#34;&#34;&#34;Possible string values and corresponding updaters object for *updater* attribute of
    `ifra.configs.NodeLearningConfig`&#34;&#34;&#34;

    possible_splitters = {}
    &#34;&#34;&#34;Possible string values and corresponding splitters object for *train_test_split* attribute of
    `ifra.configs.NodeLearningConfig`&#34;&#34;&#34;

    timeout_central = 600
    &#34;&#34;&#34;How long the node is supposed to wait for the central server to start up&#34;&#34;&#34;

    def __init__(
        self,
        learning_configs: NodeLearningConfig,
        data: NodeDataConfig,
    ):
        self.learning_configs = None
        self.data = None
        self.last_x = None
        self.last_y = None
        self.filenumber = None
        RegressionRule.rule_index += [&#34;train_test_size&#34;]
        ClassificationRule.rule_index += [&#34;train_test_size&#34;]
        super().__init__(learning_configs=learning_configs, data=data)

    @emit
    def create(self):

        self.datapreped = False
        self.copied = False
        self.splitted = False
        self.model = None
        self.last_fetch = None
        if not self.learning_configs.node_models_path.is_dir():
            self.learning_configs.node_models_path.mkdir()

        self.data.dataprep_kwargs = self.learning_configs.dataprep_kwargs

        &#34;&#34;&#34;Set fitter&#34;&#34;&#34;

        if self.learning_configs.fitter not in self.possible_fitters:
            function = self.learning_configs.fitter.split(&#34;.&#34;)[-1]
            module = self.learning_configs.fitter.replace(f&#34;.{function}&#34;, &#34;&#34;)
            self.fitter = getattr(__import__(module, globals(), locals(), [function], 0), function)(
                self.learning_configs
            )
            if not isinstance(self.fitter, Fitter):
                raise TypeError(&#34;Node fitter should inherite from Fitter class&#34;)
        else:
            self.fitter = self.possible_fitters[self.learning_configs.fitter](self.learning_configs)

        &#34;&#34;&#34;Set splitter&#34;&#34;&#34;

        if self.learning_configs.train_test_split is not None:
            if self.learning_configs.train_test_split not in self.possible_splitters:
                function = self.learning_configs.train_test_split.split(&#34;.&#34;)[-1]
                module = self.learning_configs.train_test_split.replace(f&#34;.{function}&#34;, &#34;&#34;)
                self.train_test_split = getattr(__import__(module, globals(), locals(), [function], 0), function)(
                    self.learning_configs
                )
                if not isinstance(self.train_test_split, TrainTestSplit):
                    raise TypeError(&#34;Node splitter should inherite from TrainTestSplit class&#34;)
            else:
                self.train_test_split = self.possible_splitters[self.learning_configs.train_test_split](self.data)
        else:
            self.train_test_split = TrainTestSplit(self.data)

        &#34;&#34;&#34;Set updater&#34;&#34;&#34;

        if self.learning_configs.updater not in self.possible_updaters:
            function = self.learning_configs.updater.split(&#34;.&#34;)[-1]
            module = self.learning_configs.updater.replace(f&#34;.{function}&#34;, &#34;&#34;)
            self.updater = getattr(__import__(module, globals(), locals(), [function], 0), function)(self.data)
            if not isinstance(self.updater, NodeModelUpdater):
                raise TypeError(&#34;Node updater should inherite from NodeModelUpdater class&#34;)
        else:
            self.updater = self.possible_updaters[self.learning_configs.updater](self.data)

        &#34;&#34;&#34;Set dataprep&#34;&#34;&#34;

        if self.learning_configs.dataprep is not None:
            if self.learning_configs.dataprep not in self.possible_datapreps:
                function = self.learning_configs.dataprep.split(&#34;.&#34;)[-1]
                module = self.learning_configs.dataprep.replace(f&#34;.{function}&#34;, &#34;&#34;)
                self.dataprep = getattr(__import__(module, globals(), locals(), [function], 0), function)(self.data)
                if not isinstance(self.dataprep, DataPrep):
                    raise TypeError(&#34;Node dataprep should inherite from DataPrep class&#34;)
            else:
                self.dataprep = self.possible_datapreps[self.learning_configs.dataprep](
                    self.data, **self.learning_configs.dataprep_kwargs
                )
        else:
            self.dataprep = None

    @emit
    def plot_dataprep_and_split(self) -&gt; None:
        self._plot(self.data.x_path)
        self._dataprep()
        self._split()

    @emit
    def _plot(self, which_x, name=&#34;plots&#34;):
        &#34;&#34;&#34;Plots the features and classes distribution in `ifra.node.Node`&#39;s *data.x_path* and
        `ifra.node.Node`&#39;s *data.y_path* parent directories if `ifra.configs.NodeLearningConfig` *plot_data*
        is True.&#34;&#34;&#34;
        if self.learning_configs.plot_data:
            if not (which_x.parent / name).is_dir():
                (which_x.parent / name).mkdir()
            self.plot_data_histogram(which_x, which_x.parent / name)

    @emit
    def _dataprep(self):
        &#34;&#34;&#34;
        Triggers `ifra.node.Node`&#39;s *dataprep* on the features and targets if a dataprep method was
        specified, sets `ifra.node.Node` datapreped to True, plots the distributions of the datapreped data.
        &#34;&#34;&#34;
        self.last_x = self.last_y = datetime.now()

        if self.dataprep is not None and not self.datapreped:
            logger.info(f&#34;{self.learning_configs.id} - Datapreping...&#34;)
            self.dataprep.dataprep()
            self.datapreped = True
            self._plot(self.data.x_path, &#34;plots_datapreped&#34;)
            logger.info(f&#34;{self.learning_configs.id} - ...datapreping done&#34;)

    @emit
    def _split(self):
        &#34;&#34;&#34;
        Triggers `ifra.node.Node`&#39;s *train_test_split* on the features and targets,
        sets `ifra.node.Node` splitted to True, plots the distributions of the splitted data.
        &#34;&#34;&#34;
        if not self.splitted:
            logger.info(f&#34;{self.learning_configs.id} - Splitting data in train/test...&#34;)
            self.train_test_split.split(self.iterations)
            self.splitted = True
            self._plot(self.data.x_train_path, f&#34;plots_train_{self.iterations}&#34;)
            if self.data.x_train_path != self.data.x_test_path:
                self._plot(self.data.x_test_path, &#34;plots_test&#34;)
            logger.info(f&#34;{self.learning_configs.id} - ...splitting done&#34;)

    @emit
    def fit(self) -&gt; None:
        &#34;&#34;&#34;
        Calls `ifra.node.Node.plot_dataprep_and_copy`.
        Calls the fitter corresponding to `ifra.node.Node` *learning_configs.fitter* on the node&#39;s features and
        targets.
        Filters out rules that are already present in the central model, if it exists.
        Saves the resulting model in `ifra.node.Node` *learning_configs.node_models_path* if new rules were found.
        &#34;&#34;&#34;

        self.plot_dataprep_and_split()

        x = self.data.x_train_path.read(**self.data.x_read_kwargs).values
        y = load_y(self.data.y_train_path, **self.data.y_read_kwargs).values
        if self.data.x_test_path != self.data.x_train_path:
            x_test = self.data.x_test_path.read(**self.data.x_read_kwargs).values
        else:
            x_test = None
        if self.data.y_test_path != self.data.y_train_path:
            y_test = load_y(self.data.y_test_path, **self.data.y_read_kwargs).values
        else:
            y_test = y

        logger.info(f&#34;{self.learning_configs.id} - Fitting with {self.learning_configs.fitter}...&#34;)
        if len(x) == 0:
            logger.warning(f&#34;{self.learning_configs.id} - No data in node&#39;s features&#34;)
            return
        self.model = self.fitter.fit(x=x, y=y)
        if self.model is None:
            logger.warning(f&#34;{self.learning_configs.id} - Fitter could not produce a model&#34;)
            return
        central_model_path = self.learning_configs.central_model_path
        if central_model_path.isfile():
            central_model = RuleSet()
            central_model.load(central_model_path)
            rules = [r for r in self.model if r not in central_model]
            self.model = RuleSet(rules)

        if len(self.model) &gt; 0:
            self.coverage = self.model.ruleset_coverage
            self.model.eval(
                xs=x_test, y=y_test, keep_new_activations=x_test is not None, **self.learning_configs.eval_kwargs
            )
            # noinspection PyProtectedMember
            if self.model.criterion is None:
                self.model.criterion = np.nan
            logger.info(
                f&#34;{self.learning_configs.id} - Found {len(self.model)} new good rules.&#34;
                f&#34; Criterion is {round(self.model.criterion, 3)}, coverage is {round(self.coverage, 3)}&#34;
            )
            self.model_to_file()
            self.fitter.save(self.learning_configs.node_models_path / f&#34;model_{self.filenumber}_{self.iterations}&#34;)
        else:
            logger.info(f&#34;{self.learning_configs.id} - Did not find rules&#34;)
        if self.model is not None and self.learning_configs.privacy_proba is not None:
            apply_diff_privacy(
                ruleset=self.model, y=y, p=self.learning_configs.privacy_proba, name=self.learning_configs.id
            )
            if len(self.model) == 0:
                logger.warning(
                    f&#34;{self.learning_configs.id} - No good rule left in model after applying differential privacy&#34;
                )
                self.model = None

    @emit
    def update_from_central(self, model: RuleSet) -&gt; None:
        &#34;&#34;&#34;Modifies the files pointed by `ifra.node.Node`&#39;s *data.x_path* and `ifra.node.Node`&#39;s *data.y_path* by
        calling `ifra.node.Node` *learning_configs.updater*.

        Parameters
        ----------
        model: RuleSet
            The central model
        &#34;&#34;&#34;
        logger.info(f&#34;{self.learning_configs.id} - Updating node...&#34;)
        # Compute activation of the selected rules
        self.updater.update(model)
        logger.info(f&#34;{self.learning_configs.id} - ... node updated.&#34;)

    @emit
    def model_to_file(self) -&gt; None:
        &#34;&#34;&#34;Saves `ifra.node.Node` *model* to `ifra.configs.NodeLearningConfig` *node_models_path*,
        under &#39;model_nnode_niteration.csv&#39; and &#39;model_main_nnode&#39;, where &#39;nnode&#39; is determined by counting how many
        files named &#39;model_*_0.csv&#39; already exist, and where &#39;niteration&#39; is incremented at each call of this method,
        starting at 0.
        Will also produce a .pdf of the model using TableWriter if LaTeX is available on the system.
        Does not do anything if `ifra.node.Node` *model* is None
        &#34;&#34;&#34;

        model = self.model
        if model is None:
            return

        if not self.learning_configs.node_models_path.isdir():
            self.learning_configs.node_models_path.mkdir(parents=True)
        existing_files = list(self.learning_configs.node_models_path.glob(&#34;model_main_*.csv&#34;))
        if self.filenumber is None:
            self.filenumber = random.randint(0, int(1e6))
            path_main = self.learning_configs.node_models_path / f&#34;model_main_{self.filenumber}.csv&#34;
            while path_main in existing_files:
                self.filenumber += random.randint(int(2e6), int(3e6))
                path_main = self.learning_configs.node_models_path / f&#34;model_main_{self.filenumber}.csv&#34;
            path_main.touch()  # To lock file for possible other nodes trying to write here
        else:
            path_main = self.learning_configs.node_models_path / f&#34;model_main_{self.filenumber}.csv&#34;

        path_iteration = self.learning_configs.node_models_path / f&#34;model_{self.filenumber}_{self.iterations}.csv&#34;
        self.iterations += 1

        if not path_main.parent.isdir():
            path_main.parent.mkdir(parents=True)
        model.save(path_main)
        model.save(path_iteration)

        logger.info(f&#34;{self.learning_configs.id} - Node&#39;s model saved in {path_iteration} and {path_main}.&#34;)

        try:
            path_table = path_main.with_suffix(&#34;.pdf&#34;)
            df = path_main.read(index_col=0)
            if not df.empty:
                df = df.apply(lambda x: x.round(3) if x.dtype == float else x)
            else:
                df = pd.DataFrame(data=[[&#34;empty&#34;]])
            TableWriter(path_table, df, paperwidth=30).compile(clean_tex=True)
        except ValueError:
            logger.warning(f&#34;{self.learning_configs.id} - Failed to produce tablewriter. Is LaTeX installed ?&#34;)

    @emit
    def plot_data_histogram(self, x_path: TransparentPath, output_path: TransparentPath) -&gt; None:
        &#34;&#34;&#34;Plots the distribution of the data located in `ifra.node.Node`&#39;s *data.x_path* and
        `ifra.node.Node`&#39;s *data.y_path* and save them in unique files.

        Parameters
        ----------
        x_path: TransparentPath
            Path of the x data to plot. Must be a file.
        output_path: TransparentPath
            Path to save the data. Should be a directory.
        &#34;&#34;&#34;
        x = x_path.read(**self.data.x_read_kwargs)
        for col, name in zip(x.columns, self.learning_configs.features_names):
            fig = plot_histogram(data=x[col], xlabel=name, figsize=(10, 7))

            iteration = 0
            name = name.replace(&#34; &#34;, &#34;_&#34;)
            path_x = output_path / f&#34;{name}_{iteration}.pdf&#34;
            while path_x.is_file():
                iteration += 1
                path_x = path_x.parent / f&#34;{name}_{iteration}.pdf&#34;
            fig.savefig(path_x)

        y = load_y(self.data.y_path, **self.data.y_read_kwargs)
        fig = plot_histogram(data=y.squeeze(), xlabel=&#34;Class&#34;, figsize=(10, 7))

        iteration = 0
        path_y = output_path / f&#34;classes_{iteration}.pdf&#34;
        while path_y.is_file():
            iteration += 1
            path_y = path_y.parent / f&#34;classes_{iteration}.pdf&#34;

        fig.savefig(path_y)

    @emit
    def run(self, timeout: Union[int, float] = 0, sleeptime: Union[int, float] = 5) -&gt; Tuple[int, int]:
        &#34;&#34;&#34;Monitors new changes in the central server, every *sleeptime* seconds for *timeout* seconds, triggering
        node fit when a new model is found, if new data are available or if the function just started. Sets
        `ifra.configs.NodeLearningConfig` *id* and *central_server_path* by re-reading the configuration file.

        If central model is not present, waits until it is or until new data are available.
        This is the only method the user should call.

        Parameters
        ----------
        timeout: Union[int, float]
            How many seconds the run last. If &lt;= 0&gt;, will last until killed by the user. Default value = 0
        sleeptime: Union[int, float]
            How many seconds between each checks for new central model. Default value = 5

        Returns
        -------
        Tuple[int, int]
            Random number used to name the node&#39;s model file, and the number of iterations done
        &#34;&#34;&#34;
        # noinspection PyUnusedLocal
        @emit  # Let self_ here for proper use of &#39;emit&#39;
        def get_model(self_) -&gt; None:
            &#34;&#34;&#34;Fetch the central server&#39;s latest model.
            `ifra.node.Node.last_fetch` will be set to now.&#34;&#34;&#34;
            central_model = RuleSet()
            central_model.load(self.learning_configs.central_model_path)
            self.last_fetch = datetime.now()
            self.update_from_central(central_model)
            logger.info(f&#34;{self.learning_configs.id} - Fetched central model at {self.last_fetch}&#34;)

        # start at true to trigger fit even if no central model is here at first iteration
        do_fit = self.iterations == 0
        self.plot_dataprep_and_split()  # make dataprep at run start
        started = False  # To force at least one loop of the while to trigger

        if timeout &lt;= 0:
            logger.warning(
                f&#34;{self.learning_configs.id} -&#34;
                &#34; You did not specify a timeout for your run. It will last until manually stopped.&#34;
            )

        logger.info(f&#34;{self.learning_configs.id} - Starting run&#34;)
        logger.info(
            f&#34;Starting node {self.learning_configs.id}. Monitoring changes in &#34;
            f&#34;{self.learning_configs.central_model_path}, {self.data.x_path} and {self.data.y_path}.&#34;
        )

        t = time()
        while time() - t &lt; timeout or timeout &lt;= 0 or started is False:
            started = True
            if (
                self.last_x is not None
                and self.last_y is not None
                and (
                    self.data.x_path.info()[&#34;mtime&#34;] &gt; self.last_x.timestamp()
                    or self.data.y_path.info()[&#34;mtime&#34;] &gt; self.last_y.timestamp()
                )
            ):
                # New data arrived for the node to use : redo dataprep and copy and force update from the central model
                logger.info(f&#34;{self.learning_configs.id} - New data available&#34;)
                self.datapreped = False
                self.splitted = False
                self.last_fetch = None
                self.plot_dataprep_and_split()

            self.learning_configs = NodeLearningConfig(self.learning_configs.path)  # In case NodeGate changed something

            if self.last_fetch is None:
                if self.learning_configs.central_model_path.is_file():
                    get_model(self)
                    do_fit = True
            else:
                if (
                    self.learning_configs.central_model_path.is_file()
                    and self.learning_configs.central_model_path.info()[&#34;mtime&#34;] &gt; self.last_fetch.timestamp()
                ):
                    get_model(self)
                    do_fit = True

            if do_fit:
                self.fit()
                do_fit = False
            sleep(sleeptime)

        logger.info(f&#34;{self.learning_configs.id} - Timeout of {timeout} seconds reached, stopping learning.&#34;)

        # Returned values can be used to test the code locally
        return self.filenumber, self.iterations</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ifra.node.Node"><code class="flex name class">
<span>class <span class="ident">Node</span></span>
<span>(</span><span>learning_configs: <a title="ifra.configs.NodeLearningConfig" href="configs.html#ifra.configs.NodeLearningConfig">NodeLearningConfig</a>, data: <a title="ifra.configs.NodeDataConfig" href="configs.html#ifra.configs.NodeDataConfig">NodeDataConfig</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>One node of federated learning.
This class should be used by each machine that is supposed to produce a local
model. It monitors changes in a directory where the central server is supposed to push its model, and triggers fit
when a new central model is available. It will also trigger a first fit before any central model is available. It
also monitors changes in the input data file, to automatically re-fit the model when new data are available.
Any rules produced by the node and already present in the central model are ignored.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>learning_configs</code></strong> :&ensp;<code><a title="ifra.configs.NodeLearningConfig" href="configs.html#ifra.configs.NodeLearningConfig">NodeLearningConfig</a></code></dt>
<dd>The learning configuration of the node. Will be accessible by the central server.
see <code><a title="ifra.configs.NodeLearningConfig" href="configs.html#ifra.configs.NodeLearningConfig">NodeLearningConfig</a></code></dd>
<dt><strong><code>path_learning_configs</code></strong> :&ensp;<code>TransparentPath</code></dt>
<dd>Path to the json file containing learning configuration of the node. Needs to be kept in memory to potentially
update the node's id once set by the central server.</dd>
<dt><strong><code>emitter</code></strong> :&ensp;<code>Emitter</code></dt>
<dd><code><a title="ifra.messenger.Emitter" href="messenger.html#ifra.messenger.Emitter">Emitter</a></code></dd>
<dt><strong><code>dataprep</code></strong> :&ensp;<code>Union[None, Callable]</code></dt>
<dd>The dataprep method to apply to the data before fitting. Optional, specified in the node's learning
configuration.</dd>
<dt><strong><code>datapreped</code></strong> :&ensp;<code>bool</code></dt>
<dd>False if the dataprep has not been done yet, True after</dd>
<dt><strong><code>splitted</code></strong> :&ensp;<code>bool</code></dt>
<dd>This flag is True if the features and targets have been splitted into train/test yet, False otherwise.</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>Union[None, RuleSet]</code></dt>
<dd>Fitted model</dd>
<dt><strong><code>last_fetch</code></strong> :&ensp;<code>Union[None, datetime]</code></dt>
<dd>Date and time when the central model was last fetched.</dd>
<dt><strong><code>last_x</code></strong> :&ensp;<code>Union[None, datetime]</code></dt>
<dd>Date and time when the x data were last modified.</dd>
<dt><strong><code>last_y</code></strong> :&ensp;<code>Union[None, datetime]</code></dt>
<dd>Date and time when the y data were last modified.</dd>
<dt><strong><code>data</code></strong> :&ensp;<code><a title="ifra.configs.NodeDataConfig" href="configs.html#ifra.configs.NodeDataConfig">NodeDataConfig</a></code></dt>
<dd>Configuration of the paths to the node's features and target data files</dd>
<dt><strong><code>fitter</code></strong> :&ensp;<code><a title="ifra.fitters.Fitter" href="fitters.html#ifra.fitters.Fitter">Fitter</a></code></dt>
<dd>Configuration of the paths to the node's features and target data files</dd>
<dt><strong><code>updater</code></strong> :&ensp;<code><a title="ifra.node_model_updaters.NodeModelUpdater" href="node_model_updaters.html#ifra.node_model_updaters.NodeModelUpdater">NodeModelUpdater</a></code></dt>
<dd>Configuration of the paths to the node's features and target data files</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Node(Actor):
    # noinspection PyUnresolvedReferences
    &#34;&#34;&#34;One node of federated learning.
    This class should be used by each machine that is supposed to produce a local
    model. It monitors changes in a directory where the central server is supposed to push its model, and triggers fit
    when a new central model is available. It will also trigger a first fit before any central model is available. It
    also monitors changes in the input data file, to automatically re-fit the model when new data are available.
    Any rules produced by the node and already present in the central model are ignored.

    Attributes
    ----------
    learning_configs: `ifra.configs.NodeLearningConfig`
        The learning configuration of the node. Will be accessible by the central server.
        see `ifra.configs.NodeLearningConfig`
    path_learning_configs: TransparentPath
        Path to the json file containing learning configuration of the node. Needs to be kept in memory to potentially
        update the node&#39;s id once set by the central server.
    emitter: Emitter
        `ifra.messenger.Emitter`
    dataprep: Union[None, Callable]
        The dataprep method to apply to the data before fitting. Optional, specified in the node&#39;s learning
        configuration.
    datapreped: bool
        False if the dataprep has not been done yet, True after
    splitted: bool
        This flag is True if the features and targets have been splitted into train/test yet, False otherwise.
    model: Union[None, RuleSet]
        Fitted model
    last_fetch: Union[None, datetime]
        Date and time when the central model was last fetched.
    last_x: Union[None, datetime]
        Date and time when the x data were last modified.
    last_y: Union[None, datetime]
        Date and time when the y data were last modified.
    data: `ifra.configs.NodeDataConfig`
        Configuration of the paths to the node&#39;s features and target data files
    fitter: `ifra.fitters.Fitter`
        Configuration of the paths to the node&#39;s features and target data files
    updater: `ifra.node_model_updaters.NodeModelUpdater`
        Configuration of the paths to the node&#39;s features and target data files
    &#34;&#34;&#34;

    possible_fitters = {
        &#34;decisiontreeclassification&#34;: DecisionTreeClassificationFitter,
        &#34;decisiontreeregression&#34;: DecisionTreeRegressionFitter,
    }
    &#34;&#34;&#34;Possible string values and corresponding fitter object for *fitter* attribute of
    `ifra.configs.NodeLearningConfig`&#34;&#34;&#34;

    possible_updaters = {&#34;adaboost&#34;: AdaBoostNodeModelUpdater}
    &#34;&#34;&#34;Possible string values and corresponding updaters object for *updater* attribute of
    `ifra.configs.NodeLearningConfig`&#34;&#34;&#34;

    possible_datapreps = {&#34;binfeatures&#34;: BinFeaturesDataPrep}
    &#34;&#34;&#34;Possible string values and corresponding updaters object for *updater* attribute of
    `ifra.configs.NodeLearningConfig`&#34;&#34;&#34;

    possible_splitters = {}
    &#34;&#34;&#34;Possible string values and corresponding splitters object for *train_test_split* attribute of
    `ifra.configs.NodeLearningConfig`&#34;&#34;&#34;

    timeout_central = 600
    &#34;&#34;&#34;How long the node is supposed to wait for the central server to start up&#34;&#34;&#34;

    def __init__(
        self,
        learning_configs: NodeLearningConfig,
        data: NodeDataConfig,
    ):
        self.learning_configs = None
        self.data = None
        self.last_x = None
        self.last_y = None
        self.filenumber = None
        RegressionRule.rule_index += [&#34;train_test_size&#34;]
        ClassificationRule.rule_index += [&#34;train_test_size&#34;]
        super().__init__(learning_configs=learning_configs, data=data)

    @emit
    def create(self):

        self.datapreped = False
        self.copied = False
        self.splitted = False
        self.model = None
        self.last_fetch = None
        if not self.learning_configs.node_models_path.is_dir():
            self.learning_configs.node_models_path.mkdir()

        self.data.dataprep_kwargs = self.learning_configs.dataprep_kwargs

        &#34;&#34;&#34;Set fitter&#34;&#34;&#34;

        if self.learning_configs.fitter not in self.possible_fitters:
            function = self.learning_configs.fitter.split(&#34;.&#34;)[-1]
            module = self.learning_configs.fitter.replace(f&#34;.{function}&#34;, &#34;&#34;)
            self.fitter = getattr(__import__(module, globals(), locals(), [function], 0), function)(
                self.learning_configs
            )
            if not isinstance(self.fitter, Fitter):
                raise TypeError(&#34;Node fitter should inherite from Fitter class&#34;)
        else:
            self.fitter = self.possible_fitters[self.learning_configs.fitter](self.learning_configs)

        &#34;&#34;&#34;Set splitter&#34;&#34;&#34;

        if self.learning_configs.train_test_split is not None:
            if self.learning_configs.train_test_split not in self.possible_splitters:
                function = self.learning_configs.train_test_split.split(&#34;.&#34;)[-1]
                module = self.learning_configs.train_test_split.replace(f&#34;.{function}&#34;, &#34;&#34;)
                self.train_test_split = getattr(__import__(module, globals(), locals(), [function], 0), function)(
                    self.learning_configs
                )
                if not isinstance(self.train_test_split, TrainTestSplit):
                    raise TypeError(&#34;Node splitter should inherite from TrainTestSplit class&#34;)
            else:
                self.train_test_split = self.possible_splitters[self.learning_configs.train_test_split](self.data)
        else:
            self.train_test_split = TrainTestSplit(self.data)

        &#34;&#34;&#34;Set updater&#34;&#34;&#34;

        if self.learning_configs.updater not in self.possible_updaters:
            function = self.learning_configs.updater.split(&#34;.&#34;)[-1]
            module = self.learning_configs.updater.replace(f&#34;.{function}&#34;, &#34;&#34;)
            self.updater = getattr(__import__(module, globals(), locals(), [function], 0), function)(self.data)
            if not isinstance(self.updater, NodeModelUpdater):
                raise TypeError(&#34;Node updater should inherite from NodeModelUpdater class&#34;)
        else:
            self.updater = self.possible_updaters[self.learning_configs.updater](self.data)

        &#34;&#34;&#34;Set dataprep&#34;&#34;&#34;

        if self.learning_configs.dataprep is not None:
            if self.learning_configs.dataprep not in self.possible_datapreps:
                function = self.learning_configs.dataprep.split(&#34;.&#34;)[-1]
                module = self.learning_configs.dataprep.replace(f&#34;.{function}&#34;, &#34;&#34;)
                self.dataprep = getattr(__import__(module, globals(), locals(), [function], 0), function)(self.data)
                if not isinstance(self.dataprep, DataPrep):
                    raise TypeError(&#34;Node dataprep should inherite from DataPrep class&#34;)
            else:
                self.dataprep = self.possible_datapreps[self.learning_configs.dataprep](
                    self.data, **self.learning_configs.dataprep_kwargs
                )
        else:
            self.dataprep = None

    @emit
    def plot_dataprep_and_split(self) -&gt; None:
        self._plot(self.data.x_path)
        self._dataprep()
        self._split()

    @emit
    def _plot(self, which_x, name=&#34;plots&#34;):
        &#34;&#34;&#34;Plots the features and classes distribution in `ifra.node.Node`&#39;s *data.x_path* and
        `ifra.node.Node`&#39;s *data.y_path* parent directories if `ifra.configs.NodeLearningConfig` *plot_data*
        is True.&#34;&#34;&#34;
        if self.learning_configs.plot_data:
            if not (which_x.parent / name).is_dir():
                (which_x.parent / name).mkdir()
            self.plot_data_histogram(which_x, which_x.parent / name)

    @emit
    def _dataprep(self):
        &#34;&#34;&#34;
        Triggers `ifra.node.Node`&#39;s *dataprep* on the features and targets if a dataprep method was
        specified, sets `ifra.node.Node` datapreped to True, plots the distributions of the datapreped data.
        &#34;&#34;&#34;
        self.last_x = self.last_y = datetime.now()

        if self.dataprep is not None and not self.datapreped:
            logger.info(f&#34;{self.learning_configs.id} - Datapreping...&#34;)
            self.dataprep.dataprep()
            self.datapreped = True
            self._plot(self.data.x_path, &#34;plots_datapreped&#34;)
            logger.info(f&#34;{self.learning_configs.id} - ...datapreping done&#34;)

    @emit
    def _split(self):
        &#34;&#34;&#34;
        Triggers `ifra.node.Node`&#39;s *train_test_split* on the features and targets,
        sets `ifra.node.Node` splitted to True, plots the distributions of the splitted data.
        &#34;&#34;&#34;
        if not self.splitted:
            logger.info(f&#34;{self.learning_configs.id} - Splitting data in train/test...&#34;)
            self.train_test_split.split(self.iterations)
            self.splitted = True
            self._plot(self.data.x_train_path, f&#34;plots_train_{self.iterations}&#34;)
            if self.data.x_train_path != self.data.x_test_path:
                self._plot(self.data.x_test_path, &#34;plots_test&#34;)
            logger.info(f&#34;{self.learning_configs.id} - ...splitting done&#34;)

    @emit
    def fit(self) -&gt; None:
        &#34;&#34;&#34;
        Calls `ifra.node.Node.plot_dataprep_and_copy`.
        Calls the fitter corresponding to `ifra.node.Node` *learning_configs.fitter* on the node&#39;s features and
        targets.
        Filters out rules that are already present in the central model, if it exists.
        Saves the resulting model in `ifra.node.Node` *learning_configs.node_models_path* if new rules were found.
        &#34;&#34;&#34;

        self.plot_dataprep_and_split()

        x = self.data.x_train_path.read(**self.data.x_read_kwargs).values
        y = load_y(self.data.y_train_path, **self.data.y_read_kwargs).values
        if self.data.x_test_path != self.data.x_train_path:
            x_test = self.data.x_test_path.read(**self.data.x_read_kwargs).values
        else:
            x_test = None
        if self.data.y_test_path != self.data.y_train_path:
            y_test = load_y(self.data.y_test_path, **self.data.y_read_kwargs).values
        else:
            y_test = y

        logger.info(f&#34;{self.learning_configs.id} - Fitting with {self.learning_configs.fitter}...&#34;)
        if len(x) == 0:
            logger.warning(f&#34;{self.learning_configs.id} - No data in node&#39;s features&#34;)
            return
        self.model = self.fitter.fit(x=x, y=y)
        if self.model is None:
            logger.warning(f&#34;{self.learning_configs.id} - Fitter could not produce a model&#34;)
            return
        central_model_path = self.learning_configs.central_model_path
        if central_model_path.isfile():
            central_model = RuleSet()
            central_model.load(central_model_path)
            rules = [r for r in self.model if r not in central_model]
            self.model = RuleSet(rules)

        if len(self.model) &gt; 0:
            self.coverage = self.model.ruleset_coverage
            self.model.eval(
                xs=x_test, y=y_test, keep_new_activations=x_test is not None, **self.learning_configs.eval_kwargs
            )
            # noinspection PyProtectedMember
            if self.model.criterion is None:
                self.model.criterion = np.nan
            logger.info(
                f&#34;{self.learning_configs.id} - Found {len(self.model)} new good rules.&#34;
                f&#34; Criterion is {round(self.model.criterion, 3)}, coverage is {round(self.coverage, 3)}&#34;
            )
            self.model_to_file()
            self.fitter.save(self.learning_configs.node_models_path / f&#34;model_{self.filenumber}_{self.iterations}&#34;)
        else:
            logger.info(f&#34;{self.learning_configs.id} - Did not find rules&#34;)
        if self.model is not None and self.learning_configs.privacy_proba is not None:
            apply_diff_privacy(
                ruleset=self.model, y=y, p=self.learning_configs.privacy_proba, name=self.learning_configs.id
            )
            if len(self.model) == 0:
                logger.warning(
                    f&#34;{self.learning_configs.id} - No good rule left in model after applying differential privacy&#34;
                )
                self.model = None

    @emit
    def update_from_central(self, model: RuleSet) -&gt; None:
        &#34;&#34;&#34;Modifies the files pointed by `ifra.node.Node`&#39;s *data.x_path* and `ifra.node.Node`&#39;s *data.y_path* by
        calling `ifra.node.Node` *learning_configs.updater*.

        Parameters
        ----------
        model: RuleSet
            The central model
        &#34;&#34;&#34;
        logger.info(f&#34;{self.learning_configs.id} - Updating node...&#34;)
        # Compute activation of the selected rules
        self.updater.update(model)
        logger.info(f&#34;{self.learning_configs.id} - ... node updated.&#34;)

    @emit
    def model_to_file(self) -&gt; None:
        &#34;&#34;&#34;Saves `ifra.node.Node` *model* to `ifra.configs.NodeLearningConfig` *node_models_path*,
        under &#39;model_nnode_niteration.csv&#39; and &#39;model_main_nnode&#39;, where &#39;nnode&#39; is determined by counting how many
        files named &#39;model_*_0.csv&#39; already exist, and where &#39;niteration&#39; is incremented at each call of this method,
        starting at 0.
        Will also produce a .pdf of the model using TableWriter if LaTeX is available on the system.
        Does not do anything if `ifra.node.Node` *model* is None
        &#34;&#34;&#34;

        model = self.model
        if model is None:
            return

        if not self.learning_configs.node_models_path.isdir():
            self.learning_configs.node_models_path.mkdir(parents=True)
        existing_files = list(self.learning_configs.node_models_path.glob(&#34;model_main_*.csv&#34;))
        if self.filenumber is None:
            self.filenumber = random.randint(0, int(1e6))
            path_main = self.learning_configs.node_models_path / f&#34;model_main_{self.filenumber}.csv&#34;
            while path_main in existing_files:
                self.filenumber += random.randint(int(2e6), int(3e6))
                path_main = self.learning_configs.node_models_path / f&#34;model_main_{self.filenumber}.csv&#34;
            path_main.touch()  # To lock file for possible other nodes trying to write here
        else:
            path_main = self.learning_configs.node_models_path / f&#34;model_main_{self.filenumber}.csv&#34;

        path_iteration = self.learning_configs.node_models_path / f&#34;model_{self.filenumber}_{self.iterations}.csv&#34;
        self.iterations += 1

        if not path_main.parent.isdir():
            path_main.parent.mkdir(parents=True)
        model.save(path_main)
        model.save(path_iteration)

        logger.info(f&#34;{self.learning_configs.id} - Node&#39;s model saved in {path_iteration} and {path_main}.&#34;)

        try:
            path_table = path_main.with_suffix(&#34;.pdf&#34;)
            df = path_main.read(index_col=0)
            if not df.empty:
                df = df.apply(lambda x: x.round(3) if x.dtype == float else x)
            else:
                df = pd.DataFrame(data=[[&#34;empty&#34;]])
            TableWriter(path_table, df, paperwidth=30).compile(clean_tex=True)
        except ValueError:
            logger.warning(f&#34;{self.learning_configs.id} - Failed to produce tablewriter. Is LaTeX installed ?&#34;)

    @emit
    def plot_data_histogram(self, x_path: TransparentPath, output_path: TransparentPath) -&gt; None:
        &#34;&#34;&#34;Plots the distribution of the data located in `ifra.node.Node`&#39;s *data.x_path* and
        `ifra.node.Node`&#39;s *data.y_path* and save them in unique files.

        Parameters
        ----------
        x_path: TransparentPath
            Path of the x data to plot. Must be a file.
        output_path: TransparentPath
            Path to save the data. Should be a directory.
        &#34;&#34;&#34;
        x = x_path.read(**self.data.x_read_kwargs)
        for col, name in zip(x.columns, self.learning_configs.features_names):
            fig = plot_histogram(data=x[col], xlabel=name, figsize=(10, 7))

            iteration = 0
            name = name.replace(&#34; &#34;, &#34;_&#34;)
            path_x = output_path / f&#34;{name}_{iteration}.pdf&#34;
            while path_x.is_file():
                iteration += 1
                path_x = path_x.parent / f&#34;{name}_{iteration}.pdf&#34;
            fig.savefig(path_x)

        y = load_y(self.data.y_path, **self.data.y_read_kwargs)
        fig = plot_histogram(data=y.squeeze(), xlabel=&#34;Class&#34;, figsize=(10, 7))

        iteration = 0
        path_y = output_path / f&#34;classes_{iteration}.pdf&#34;
        while path_y.is_file():
            iteration += 1
            path_y = path_y.parent / f&#34;classes_{iteration}.pdf&#34;

        fig.savefig(path_y)

    @emit
    def run(self, timeout: Union[int, float] = 0, sleeptime: Union[int, float] = 5) -&gt; Tuple[int, int]:
        &#34;&#34;&#34;Monitors new changes in the central server, every *sleeptime* seconds for *timeout* seconds, triggering
        node fit when a new model is found, if new data are available or if the function just started. Sets
        `ifra.configs.NodeLearningConfig` *id* and *central_server_path* by re-reading the configuration file.

        If central model is not present, waits until it is or until new data are available.
        This is the only method the user should call.

        Parameters
        ----------
        timeout: Union[int, float]
            How many seconds the run last. If &lt;= 0&gt;, will last until killed by the user. Default value = 0
        sleeptime: Union[int, float]
            How many seconds between each checks for new central model. Default value = 5

        Returns
        -------
        Tuple[int, int]
            Random number used to name the node&#39;s model file, and the number of iterations done
        &#34;&#34;&#34;
        # noinspection PyUnusedLocal
        @emit  # Let self_ here for proper use of &#39;emit&#39;
        def get_model(self_) -&gt; None:
            &#34;&#34;&#34;Fetch the central server&#39;s latest model.
            `ifra.node.Node.last_fetch` will be set to now.&#34;&#34;&#34;
            central_model = RuleSet()
            central_model.load(self.learning_configs.central_model_path)
            self.last_fetch = datetime.now()
            self.update_from_central(central_model)
            logger.info(f&#34;{self.learning_configs.id} - Fetched central model at {self.last_fetch}&#34;)

        # start at true to trigger fit even if no central model is here at first iteration
        do_fit = self.iterations == 0
        self.plot_dataprep_and_split()  # make dataprep at run start
        started = False  # To force at least one loop of the while to trigger

        if timeout &lt;= 0:
            logger.warning(
                f&#34;{self.learning_configs.id} -&#34;
                &#34; You did not specify a timeout for your run. It will last until manually stopped.&#34;
            )

        logger.info(f&#34;{self.learning_configs.id} - Starting run&#34;)
        logger.info(
            f&#34;Starting node {self.learning_configs.id}. Monitoring changes in &#34;
            f&#34;{self.learning_configs.central_model_path}, {self.data.x_path} and {self.data.y_path}.&#34;
        )

        t = time()
        while time() - t &lt; timeout or timeout &lt;= 0 or started is False:
            started = True
            if (
                self.last_x is not None
                and self.last_y is not None
                and (
                    self.data.x_path.info()[&#34;mtime&#34;] &gt; self.last_x.timestamp()
                    or self.data.y_path.info()[&#34;mtime&#34;] &gt; self.last_y.timestamp()
                )
            ):
                # New data arrived for the node to use : redo dataprep and copy and force update from the central model
                logger.info(f&#34;{self.learning_configs.id} - New data available&#34;)
                self.datapreped = False
                self.splitted = False
                self.last_fetch = None
                self.plot_dataprep_and_split()

            self.learning_configs = NodeLearningConfig(self.learning_configs.path)  # In case NodeGate changed something

            if self.last_fetch is None:
                if self.learning_configs.central_model_path.is_file():
                    get_model(self)
                    do_fit = True
            else:
                if (
                    self.learning_configs.central_model_path.is_file()
                    and self.learning_configs.central_model_path.info()[&#34;mtime&#34;] &gt; self.last_fetch.timestamp()
                ):
                    get_model(self)
                    do_fit = True

            if do_fit:
                self.fit()
                do_fit = False
            sleep(sleeptime)

        logger.info(f&#34;{self.learning_configs.id} - Timeout of {timeout} seconds reached, stopping learning.&#34;)

        # Returned values can be used to test the code locally
        return self.filenumber, self.iterations</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ifra.actor.Actor" href="actor.html#ifra.actor.Actor">Actor</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="ifra.node.Node.possible_datapreps"><code class="name">var <span class="ident">possible_datapreps</span></code></dt>
<dd>
<div class="desc"><p>Possible string values and corresponding updaters object for <em>updater</em> attribute of
<code><a title="ifra.configs.NodeLearningConfig" href="configs.html#ifra.configs.NodeLearningConfig">NodeLearningConfig</a></code></p></div>
</dd>
<dt id="ifra.node.Node.possible_fitters"><code class="name">var <span class="ident">possible_fitters</span></code></dt>
<dd>
<div class="desc"><p>Possible string values and corresponding fitter object for <em>fitter</em> attribute of
<code><a title="ifra.configs.NodeLearningConfig" href="configs.html#ifra.configs.NodeLearningConfig">NodeLearningConfig</a></code></p></div>
</dd>
<dt id="ifra.node.Node.possible_splitters"><code class="name">var <span class="ident">possible_splitters</span></code></dt>
<dd>
<div class="desc"><p>Possible string values and corresponding splitters object for <em>train_test_split</em> attribute of
<code><a title="ifra.configs.NodeLearningConfig" href="configs.html#ifra.configs.NodeLearningConfig">NodeLearningConfig</a></code></p></div>
</dd>
<dt id="ifra.node.Node.possible_updaters"><code class="name">var <span class="ident">possible_updaters</span></code></dt>
<dd>
<div class="desc"><p>Possible string values and corresponding updaters object for <em>updater</em> attribute of
<code><a title="ifra.configs.NodeLearningConfig" href="configs.html#ifra.configs.NodeLearningConfig">NodeLearningConfig</a></code></p></div>
</dd>
<dt id="ifra.node.Node.timeout_central"><code class="name">var <span class="ident">timeout_central</span></code></dt>
<dd>
<div class="desc"><p>How long the node is supposed to wait for the central server to start up</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ifra.node.Node.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Calls <code>ifra.node.Node.plot_dataprep_and_copy</code>.
Calls the fitter corresponding to <code><a title="ifra.node.Node" href="#ifra.node.Node">Node</a></code> <em>learning_configs.fitter</em> on the node's features and
targets.
Filters out rules that are already present in the central model, if it exists.
Saves the resulting model in <code><a title="ifra.node.Node" href="#ifra.node.Node">Node</a></code> <em>learning_configs.node_models_path</em> if new rules were found.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@emit
def fit(self) -&gt; None:
    &#34;&#34;&#34;
    Calls `ifra.node.Node.plot_dataprep_and_copy`.
    Calls the fitter corresponding to `ifra.node.Node` *learning_configs.fitter* on the node&#39;s features and
    targets.
    Filters out rules that are already present in the central model, if it exists.
    Saves the resulting model in `ifra.node.Node` *learning_configs.node_models_path* if new rules were found.
    &#34;&#34;&#34;

    self.plot_dataprep_and_split()

    x = self.data.x_train_path.read(**self.data.x_read_kwargs).values
    y = load_y(self.data.y_train_path, **self.data.y_read_kwargs).values
    if self.data.x_test_path != self.data.x_train_path:
        x_test = self.data.x_test_path.read(**self.data.x_read_kwargs).values
    else:
        x_test = None
    if self.data.y_test_path != self.data.y_train_path:
        y_test = load_y(self.data.y_test_path, **self.data.y_read_kwargs).values
    else:
        y_test = y

    logger.info(f&#34;{self.learning_configs.id} - Fitting with {self.learning_configs.fitter}...&#34;)
    if len(x) == 0:
        logger.warning(f&#34;{self.learning_configs.id} - No data in node&#39;s features&#34;)
        return
    self.model = self.fitter.fit(x=x, y=y)
    if self.model is None:
        logger.warning(f&#34;{self.learning_configs.id} - Fitter could not produce a model&#34;)
        return
    central_model_path = self.learning_configs.central_model_path
    if central_model_path.isfile():
        central_model = RuleSet()
        central_model.load(central_model_path)
        rules = [r for r in self.model if r not in central_model]
        self.model = RuleSet(rules)

    if len(self.model) &gt; 0:
        self.coverage = self.model.ruleset_coverage
        self.model.eval(
            xs=x_test, y=y_test, keep_new_activations=x_test is not None, **self.learning_configs.eval_kwargs
        )
        # noinspection PyProtectedMember
        if self.model.criterion is None:
            self.model.criterion = np.nan
        logger.info(
            f&#34;{self.learning_configs.id} - Found {len(self.model)} new good rules.&#34;
            f&#34; Criterion is {round(self.model.criterion, 3)}, coverage is {round(self.coverage, 3)}&#34;
        )
        self.model_to_file()
        self.fitter.save(self.learning_configs.node_models_path / f&#34;model_{self.filenumber}_{self.iterations}&#34;)
    else:
        logger.info(f&#34;{self.learning_configs.id} - Did not find rules&#34;)
    if self.model is not None and self.learning_configs.privacy_proba is not None:
        apply_diff_privacy(
            ruleset=self.model, y=y, p=self.learning_configs.privacy_proba, name=self.learning_configs.id
        )
        if len(self.model) == 0:
            logger.warning(
                f&#34;{self.learning_configs.id} - No good rule left in model after applying differential privacy&#34;
            )
            self.model = None</code></pre>
</details>
</dd>
<dt id="ifra.node.Node.model_to_file"><code class="name flex">
<span>def <span class="ident">model_to_file</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Saves <code><a title="ifra.node.Node" href="#ifra.node.Node">Node</a></code> <em>model</em> to <code><a title="ifra.configs.NodeLearningConfig" href="configs.html#ifra.configs.NodeLearningConfig">NodeLearningConfig</a></code> <em>node_models_path</em>,
under 'model_nnode_niteration.csv' and 'model_main_nnode', where 'nnode' is determined by counting how many
files named 'model_<em>_0.csv' already exist, and where 'niteration' is incremented at each call of this method,
starting at 0.
Will also produce a .pdf of the model using TableWriter if LaTeX is available on the system.
Does not do anything if <code><a title="ifra.node.Node" href="#ifra.node.Node">Node</a></code> </em>model* is None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@emit
def model_to_file(self) -&gt; None:
    &#34;&#34;&#34;Saves `ifra.node.Node` *model* to `ifra.configs.NodeLearningConfig` *node_models_path*,
    under &#39;model_nnode_niteration.csv&#39; and &#39;model_main_nnode&#39;, where &#39;nnode&#39; is determined by counting how many
    files named &#39;model_*_0.csv&#39; already exist, and where &#39;niteration&#39; is incremented at each call of this method,
    starting at 0.
    Will also produce a .pdf of the model using TableWriter if LaTeX is available on the system.
    Does not do anything if `ifra.node.Node` *model* is None
    &#34;&#34;&#34;

    model = self.model
    if model is None:
        return

    if not self.learning_configs.node_models_path.isdir():
        self.learning_configs.node_models_path.mkdir(parents=True)
    existing_files = list(self.learning_configs.node_models_path.glob(&#34;model_main_*.csv&#34;))
    if self.filenumber is None:
        self.filenumber = random.randint(0, int(1e6))
        path_main = self.learning_configs.node_models_path / f&#34;model_main_{self.filenumber}.csv&#34;
        while path_main in existing_files:
            self.filenumber += random.randint(int(2e6), int(3e6))
            path_main = self.learning_configs.node_models_path / f&#34;model_main_{self.filenumber}.csv&#34;
        path_main.touch()  # To lock file for possible other nodes trying to write here
    else:
        path_main = self.learning_configs.node_models_path / f&#34;model_main_{self.filenumber}.csv&#34;

    path_iteration = self.learning_configs.node_models_path / f&#34;model_{self.filenumber}_{self.iterations}.csv&#34;
    self.iterations += 1

    if not path_main.parent.isdir():
        path_main.parent.mkdir(parents=True)
    model.save(path_main)
    model.save(path_iteration)

    logger.info(f&#34;{self.learning_configs.id} - Node&#39;s model saved in {path_iteration} and {path_main}.&#34;)

    try:
        path_table = path_main.with_suffix(&#34;.pdf&#34;)
        df = path_main.read(index_col=0)
        if not df.empty:
            df = df.apply(lambda x: x.round(3) if x.dtype == float else x)
        else:
            df = pd.DataFrame(data=[[&#34;empty&#34;]])
        TableWriter(path_table, df, paperwidth=30).compile(clean_tex=True)
    except ValueError:
        logger.warning(f&#34;{self.learning_configs.id} - Failed to produce tablewriter. Is LaTeX installed ?&#34;)</code></pre>
</details>
</dd>
<dt id="ifra.node.Node.plot_data_histogram"><code class="name flex">
<span>def <span class="ident">plot_data_histogram</span></span>(<span>self, x_path: transparentpath.gcsutils.transparentpath.TransparentPath, output_path: transparentpath.gcsutils.transparentpath.TransparentPath) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Plots the distribution of the data located in <code><a title="ifra.node.Node" href="#ifra.node.Node">Node</a></code>'s <em>data.x_path</em> and
<code><a title="ifra.node.Node" href="#ifra.node.Node">Node</a></code>'s <em>data.y_path</em> and save them in unique files.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x_path</code></strong> :&ensp;<code>TransparentPath</code></dt>
<dd>Path of the x data to plot. Must be a file.</dd>
<dt><strong><code>output_path</code></strong> :&ensp;<code>TransparentPath</code></dt>
<dd>Path to save the data. Should be a directory.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@emit
def plot_data_histogram(self, x_path: TransparentPath, output_path: TransparentPath) -&gt; None:
    &#34;&#34;&#34;Plots the distribution of the data located in `ifra.node.Node`&#39;s *data.x_path* and
    `ifra.node.Node`&#39;s *data.y_path* and save them in unique files.

    Parameters
    ----------
    x_path: TransparentPath
        Path of the x data to plot. Must be a file.
    output_path: TransparentPath
        Path to save the data. Should be a directory.
    &#34;&#34;&#34;
    x = x_path.read(**self.data.x_read_kwargs)
    for col, name in zip(x.columns, self.learning_configs.features_names):
        fig = plot_histogram(data=x[col], xlabel=name, figsize=(10, 7))

        iteration = 0
        name = name.replace(&#34; &#34;, &#34;_&#34;)
        path_x = output_path / f&#34;{name}_{iteration}.pdf&#34;
        while path_x.is_file():
            iteration += 1
            path_x = path_x.parent / f&#34;{name}_{iteration}.pdf&#34;
        fig.savefig(path_x)

    y = load_y(self.data.y_path, **self.data.y_read_kwargs)
    fig = plot_histogram(data=y.squeeze(), xlabel=&#34;Class&#34;, figsize=(10, 7))

    iteration = 0
    path_y = output_path / f&#34;classes_{iteration}.pdf&#34;
    while path_y.is_file():
        iteration += 1
        path_y = path_y.parent / f&#34;classes_{iteration}.pdf&#34;

    fig.savefig(path_y)</code></pre>
</details>
</dd>
<dt id="ifra.node.Node.plot_dataprep_and_split"><code class="name flex">
<span>def <span class="ident">plot_dataprep_and_split</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@emit
def plot_dataprep_and_split(self) -&gt; None:
    self._plot(self.data.x_path)
    self._dataprep()
    self._split()</code></pre>
</details>
</dd>
<dt id="ifra.node.Node.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, timeout: Union[int, float] = 0, sleeptime: Union[int, float] = 5) ‑> Tuple[int, int]</span>
</code></dt>
<dd>
<div class="desc"><p>Monitors new changes in the central server, every <em>sleeptime</em> seconds for <em>timeout</em> seconds, triggering
node fit when a new model is found, if new data are available or if the function just started. Sets
<code><a title="ifra.configs.NodeLearningConfig" href="configs.html#ifra.configs.NodeLearningConfig">NodeLearningConfig</a></code> <em>id</em> and <em>central_server_path</em> by re-reading the configuration file.</p>
<p>If central model is not present, waits until it is or until new data are available.
This is the only method the user should call.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>timeout</code></strong> :&ensp;<code>Union[int, float]</code></dt>
<dd>How many seconds the run last. If &lt;= 0&gt;, will last until killed by the user. Default value = 0</dd>
<dt><strong><code>sleeptime</code></strong> :&ensp;<code>Union[int, float]</code></dt>
<dd>How many seconds between each checks for new central model. Default value = 5</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[int, int]</code></dt>
<dd>Random number used to name the node's model file, and the number of iterations done</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@emit
def run(self, timeout: Union[int, float] = 0, sleeptime: Union[int, float] = 5) -&gt; Tuple[int, int]:
    &#34;&#34;&#34;Monitors new changes in the central server, every *sleeptime* seconds for *timeout* seconds, triggering
    node fit when a new model is found, if new data are available or if the function just started. Sets
    `ifra.configs.NodeLearningConfig` *id* and *central_server_path* by re-reading the configuration file.

    If central model is not present, waits until it is or until new data are available.
    This is the only method the user should call.

    Parameters
    ----------
    timeout: Union[int, float]
        How many seconds the run last. If &lt;= 0&gt;, will last until killed by the user. Default value = 0
    sleeptime: Union[int, float]
        How many seconds between each checks for new central model. Default value = 5

    Returns
    -------
    Tuple[int, int]
        Random number used to name the node&#39;s model file, and the number of iterations done
    &#34;&#34;&#34;
    # noinspection PyUnusedLocal
    @emit  # Let self_ here for proper use of &#39;emit&#39;
    def get_model(self_) -&gt; None:
        &#34;&#34;&#34;Fetch the central server&#39;s latest model.
        `ifra.node.Node.last_fetch` will be set to now.&#34;&#34;&#34;
        central_model = RuleSet()
        central_model.load(self.learning_configs.central_model_path)
        self.last_fetch = datetime.now()
        self.update_from_central(central_model)
        logger.info(f&#34;{self.learning_configs.id} - Fetched central model at {self.last_fetch}&#34;)

    # start at true to trigger fit even if no central model is here at first iteration
    do_fit = self.iterations == 0
    self.plot_dataprep_and_split()  # make dataprep at run start
    started = False  # To force at least one loop of the while to trigger

    if timeout &lt;= 0:
        logger.warning(
            f&#34;{self.learning_configs.id} -&#34;
            &#34; You did not specify a timeout for your run. It will last until manually stopped.&#34;
        )

    logger.info(f&#34;{self.learning_configs.id} - Starting run&#34;)
    logger.info(
        f&#34;Starting node {self.learning_configs.id}. Monitoring changes in &#34;
        f&#34;{self.learning_configs.central_model_path}, {self.data.x_path} and {self.data.y_path}.&#34;
    )

    t = time()
    while time() - t &lt; timeout or timeout &lt;= 0 or started is False:
        started = True
        if (
            self.last_x is not None
            and self.last_y is not None
            and (
                self.data.x_path.info()[&#34;mtime&#34;] &gt; self.last_x.timestamp()
                or self.data.y_path.info()[&#34;mtime&#34;] &gt; self.last_y.timestamp()
            )
        ):
            # New data arrived for the node to use : redo dataprep and copy and force update from the central model
            logger.info(f&#34;{self.learning_configs.id} - New data available&#34;)
            self.datapreped = False
            self.splitted = False
            self.last_fetch = None
            self.plot_dataprep_and_split()

        self.learning_configs = NodeLearningConfig(self.learning_configs.path)  # In case NodeGate changed something

        if self.last_fetch is None:
            if self.learning_configs.central_model_path.is_file():
                get_model(self)
                do_fit = True
        else:
            if (
                self.learning_configs.central_model_path.is_file()
                and self.learning_configs.central_model_path.info()[&#34;mtime&#34;] &gt; self.last_fetch.timestamp()
            ):
                get_model(self)
                do_fit = True

        if do_fit:
            self.fit()
            do_fit = False
        sleep(sleeptime)

    logger.info(f&#34;{self.learning_configs.id} - Timeout of {timeout} seconds reached, stopping learning.&#34;)

    # Returned values can be used to test the code locally
    return self.filenumber, self.iterations</code></pre>
</details>
</dd>
<dt id="ifra.node.Node.update_from_central"><code class="name flex">
<span>def <span class="ident">update_from_central</span></span>(<span>self, model: ruleskit.ruleset.RuleSet) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Modifies the files pointed by <code><a title="ifra.node.Node" href="#ifra.node.Node">Node</a></code>'s <em>data.x_path</em> and <code><a title="ifra.node.Node" href="#ifra.node.Node">Node</a></code>'s <em>data.y_path</em> by
calling <code><a title="ifra.node.Node" href="#ifra.node.Node">Node</a></code> <em>learning_configs.updater</em>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>RuleSet</code></dt>
<dd>The central model</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@emit
def update_from_central(self, model: RuleSet) -&gt; None:
    &#34;&#34;&#34;Modifies the files pointed by `ifra.node.Node`&#39;s *data.x_path* and `ifra.node.Node`&#39;s *data.y_path* by
    calling `ifra.node.Node` *learning_configs.updater*.

    Parameters
    ----------
    model: RuleSet
        The central model
    &#34;&#34;&#34;
    logger.info(f&#34;{self.learning_configs.id} - Updating node...&#34;)
    # Compute activation of the selected rules
    self.updater.update(model)
    logger.info(f&#34;{self.learning_configs.id} - ... node updated.&#34;)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ifra.actor.Actor" href="actor.html#ifra.actor.Actor">Actor</a></b></code>:
<ul class="hlist">
<li><code><a title="ifra.actor.Actor.create" href="actor.html#ifra.actor.Actor.create">create</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ifra" href="index.html">ifra</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ifra.node.Node" href="#ifra.node.Node">Node</a></code></h4>
<ul class="">
<li><code><a title="ifra.node.Node.fit" href="#ifra.node.Node.fit">fit</a></code></li>
<li><code><a title="ifra.node.Node.model_to_file" href="#ifra.node.Node.model_to_file">model_to_file</a></code></li>
<li><code><a title="ifra.node.Node.plot_data_histogram" href="#ifra.node.Node.plot_data_histogram">plot_data_histogram</a></code></li>
<li><code><a title="ifra.node.Node.plot_dataprep_and_split" href="#ifra.node.Node.plot_dataprep_and_split">plot_dataprep_and_split</a></code></li>
<li><code><a title="ifra.node.Node.possible_datapreps" href="#ifra.node.Node.possible_datapreps">possible_datapreps</a></code></li>
<li><code><a title="ifra.node.Node.possible_fitters" href="#ifra.node.Node.possible_fitters">possible_fitters</a></code></li>
<li><code><a title="ifra.node.Node.possible_splitters" href="#ifra.node.Node.possible_splitters">possible_splitters</a></code></li>
<li><code><a title="ifra.node.Node.possible_updaters" href="#ifra.node.Node.possible_updaters">possible_updaters</a></code></li>
<li><code><a title="ifra.node.Node.run" href="#ifra.node.Node.run">run</a></code></li>
<li><code><a title="ifra.node.Node.timeout_central" href="#ifra.node.Node.timeout_central">timeout_central</a></code></li>
<li><code><a title="ifra.node.Node.update_from_central" href="#ifra.node.Node.update_from_central">update_from_central</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>